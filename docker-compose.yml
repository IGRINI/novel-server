x-environment-defaults: &env-vars
  ENV: ${ENV}
  LOG_LEVEL: ${LOG_LEVEL}
  DB_HOST: ${DB_HOST}
  DB_PORT: ${DB_PORT}
  DB_USER: ${DB_USER}
  DB_NAME: ${DB_NAME}
  DB_SSL_MODE: ${DB_SSL_MODE}
  REDIS_ADDR: ${REDIS_ADDR}
  REDIS_DB: ${REDIS_DB}
  JWT_ACCESS_TOKEN_TTL: ${JWT_ACCESS_TOKEN_TTL}
  JWT_REFRESH_TOKEN_TTL: ${JWT_REFRESH_TOKEN_TTL}
  SERVICE_ID: ${SERVICE_ID}
  RABBITMQ_URL: ${RABBITMQ_URL}

services:
  # API Gateway (Traefik)
  api-gateway:
    image: traefik:v2.11
    container_name: novel_api_gateway
    command:
      # --- Удаляем конфигурацию Docker Provider ---
      # - "--providers.docker=true"
      # - "--providers.docker.swarmMode=true"
      # - "--providers.docker.network=novel_stack_novel_network"
      # - "--providers.docker.exposedbydefault=false"
      # - "--providers.docker.constraints=Label(`com.docker.stack.namespace`, `novel_stack`)"
      
      # --- Добавляем конфигурацию File Provider ---
      - "--providers.file.filename=/etc/traefik/dynamic/traefik_dynamic.yml" # Путь к динамической конфигурации
      - "--providers.file.watch=true" # Включаем отслеживание изменений файла

      # --- Остальные команды Traefik (точки входа, api, логи) остаются ---
      - "--entrypoints.web.address=:8080"
      - "--entrypoints.traefik.address=:8081"
      - "--api=true"
      - "--api.dashboard=true"
      - "--api.insecure=true"
      - "--accesslog=true"
      - "--log.level=DEBUG" # Оставляем DEBUG для отладки

      # --- Добавляем конфигурацию для Prometheus Metrics ---
      - "--metrics.prometheus=true"                # Включаем endpoint /metrics
      - "--metrics.prometheus.addrouterslabels=true" # Добавляем метки роутеров
      - "--metrics.prometheus.addserviceslabels=true" # Добавляем метки сервисов
      - "--metrics.prometheus.entryPoint=traefik"    # Указываем, на какой точке входа будет /metrics (обычно отдельная, здесь используем traefik @ 8081)
    ports:
      - "28960:8080"
      - "8888:8081"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock:ro" # Оставляем на всякий случай, хотя для file provider не нужен
      # --- Подключаем файл с динамической конфигурацией --- 
      - "./traefik_dynamic.yml:/etc/traefik/dynamic/traefik_dynamic.yml:ro" # Read-only
    networks:
      - novel_network
    # labels: # labels для самого Traefik больше не нужны
      # - "traefik.enable=true" 

  # База данных PostgreSQL
  postgres:
    image: postgres:15-alpine
    container_name: novel_postgres
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD_FILE: /run/secrets/db_password
      POSTGRES_DB: ${DB_NAME}
    ports:
      # Порт публикуется только для локального доступа к БД, если нужно
      - "${DB_PORT}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    secrets:
      - db_password
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER} -d ${DB_NAME}"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - novel_network

  # Кэш Redis
  redis:
    image: redis:7-alpine
    container_name: novel_redis
    ports:
      # Порт публикуется только для локального доступа к Redis, если нужно
      - "${REDIS_PORT}:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - novel_network

  # Сервис миграций
  migrate:
    build:
      context: .
      dockerfile: deploy/docker/migrate/Dockerfile
    image: novel-migrate:latest
    deploy:
      mode: replicated-job # <<< Указываем, что это одноразовая задача
      restart_policy:
        condition: none # Не перезапускать после завершения
    environment:
      # Передаем переменные, нужные для скрипта entrypoint.sh
      DB_HOST: postgres
      DB_PORT: ${DB_PORT}
      DB_USER: ${DB_USER}
      DB_NAME: ${DB_NAME}
      DB_SSL_MODE: ${DB_SSL_MODE}
    secrets:
      - source: db_password
        target: /run/secrets/db_password # Скрипт будет читать отсюда
    depends_on:
      - postgres
    networks:
      - novel_network

  # Сервис авторизации
  auth-service:
    build:
      context: .
      dockerfile: auth/Dockerfile
    image: novel-auth-service:latest
    container_name: novel_auth
    depends_on:
      - postgres
      - redis
      - migrate
    environment:
      <<: *env-vars
      DB_HOST: postgres
      REDIS_ADDR: redis:6379
      SERVER_PORT: ${AUTH_SERVER_PORT}
      SERVICE_ID: auth-service
      INTER_SERVICE_TOKEN_TTL: ${INTER_SERVICE_TOKEN_TTL}
      DB_MAX_CONNECTIONS: ${DB_MAX_CONNECTIONS}
      DB_MAX_IDLE_MINUTES: ${DB_MAX_IDLE_MINUTES}
    secrets:
      - source: db_password
        target: /run/secrets/db_password
      - source: jwt_secret
        target: /run/secrets/jwt_secret
      - source: password_pepper
        target: /run/secrets/password_pepper
      - source: inter_service_secret
        target: /run/secrets/inter_service_secret
    healthcheck: # <<< ВОЗВРАЩАЕМ HEALTHCHECK
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8081/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s # Даем 30 секунд на старт перед первыми проверками
    ports:
      - "${AUTH_SERVER_PORT}:8081"
    # labels: # <<< УДАЛЯЕМ LABELS ПОЛНОСТЬЮ
    networks:
      - novel_network

  # Сервис Геймплея
  gameplay-service:
    build:
      context: .
      dockerfile: gameplay-service/Dockerfile
    image: novel-gameplay-service:latest
    container_name: novel_gameplay_service
    depends_on:
      - postgres
      - rabbitmq
      - migrate
      - auth-service # Добавим зависимость от auth на всякий случай
    environment:
      <<: *env-vars # Наследуем общие переменные
      DB_HOST: postgres # Внутреннее имя хоста БД
      RABBITMQ_URL: amqp://${RABBITMQ_USER}:${RABBITMQ_PASS}@rabbitmq:5672/ # URL для Docker
      SERVER_PORT: ${GAMEPLAY_SERVER_PORT} # Внутренний порт сервиса
      SERVICE_ID: gameplay-service
      AUTH_SERVICE_URL: http://auth-service:8081 # Добавляем URL для auth-service
      STORY_PREVIEW_PROMPT_STYLE_SUFFIX: ${STORY_PREVIEW_PROMPT_STYLE_SUFFIX} # <<< Добавлено
    secrets:
      - source: db_password
        target: /run/secrets/db_password
      - source: jwt_secret
        target: /run/secrets/jwt_secret
      - source: inter_service_secret
        target: /run/secrets/inter_service_secret
    # <<< Добавляем Healthcheck с GET запросом >>>
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:${GAMEPLAY_SERVER_PORT}/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s # Даем время на старт
    # ports: # Убираем публикацию порта наружу
    #   - "${GAMEPLAY_SERVER_PORT:-8082}:${GAMEPLAY_SERVER_PORT:-8082}"
    # labels: # <<< УДАЛЯЕМ LABELS ПОЛНОСТЬЮ
    networks:
      - novel_network

  # Сервис Админки
  admin-service:
    build:
      context: .
      dockerfile: admin-service/Dockerfile
    image: novel-admin-service:latest
    container_name: novel_admin_service
    ports:
      - "8084:8084" # Порт admin-service
    environment:
      <<: *env-vars
      ADMIN_SERVER_PORT: 8084
      LOG_LEVEL: ${LOG_LEVEL:-debug}
      AUTH_SERVICE_URL: http://auth-service:8081
      STORY_GENERATOR_URL: http://story-generator:8083
      GAMEPLAY_SERVICE_URL: http://gameplay-service:8082
      HTTP_CLIENT_TIMEOUT: ${HTTP_CLIENT_TIMEOUT:-10s}
      AUTH_SERVICE_TIMEOUT: ${AUTH_SERVICE_TIMEOUT:-5s}
      SERVICE_ID: admin-service
      RABBITMQ_URL: amqp://${RABBITMQ_USER}:${RABBITMQ_PASS}@rabbitmq:5672/
      PUSH_QUEUE_NAME: ${PUSH_QUEUE_NAME:-push_notifications}
    secrets:
      - source: inter_service_secret
        target: /run/secrets/inter_service_secret
      - source: jwt_secret
        target: /run/secrets/jwt_secret
    depends_on:
      - auth-service
      - rabbitmq
    networks:
      - novel_network
    healthcheck: # <<< Возвращаем правильный Healthcheck для admin-service
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8084/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s # Даем время на запуск
    volumes:
      - ./admin-service:/app/admin-service # Volume mounting for hot-reloading
      - ./shared:/app/shared # Mount shared directory
    # labels: # <<< УДАЛЯЕМ LABELS ПОЛНОСТЬЮ
      # --- ВРЕМЕННО УБИРАЕМ ВСЕ МЕТКИ --- 
      # - "traefik.enable=true"
      # - "traefik.http.routers.admin-login-http.rule=Path(`/login`)" # Точный путь /login
      # - "traefik.http.routers.admin-login-http.entrypoints=web"
      # - "traefik.http.services.admin-service.loadbalancer.server.port=${ADMIN_SERVER_PORT:-8084}"
      # - "traefik.http.routers.admin-protected-http.rule=PathPrefix(`/admin`)" # Пути, начинающиеся с /admin
      # - "traefik.http.routers.admin-protected-http.entrypoints=web"
      # - "traefik.http.routers.admin-protected-http.middlewares=admin-stripprefix"

  # Сервис RabbitMQ
  rabbitmq:
    image: rabbitmq:3.13-management-alpine
    container_name: novel_rabbitmq
    ports:
      # Порты для доступа извне (если нужно) и для Management UI
      - "${RABBITMQ_PORT}:5672"
      - "${RABBITMQ_MANAGE_PORT}:15672"
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq/
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER}
    healthcheck:
      test: rabbitmq-diagnostics -q ping
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - novel_network

  # Сервис Story Generator
  story-generator:
    build:
      context: .
      dockerfile: story-generator/Dockerfile
    image: novel-story-generator:latest
    container_name: novel_story_generator
    depends_on:
      - postgres
      - rabbitmq
      - migrate
    volumes:
      - ./prompts:/app/prompts
    environment:
      <<: *env-vars
      DB_HOST: postgres
      RABBITMQ_URL: amqp://${RABBITMQ_USER}:${RABBITMQ_PASS}@rabbitmq:5672/
      PROMPTS_DIR: /app/prompts
      SERVICE_ID: story-generator-service
      AI_BASE_URL: ${AI_BASE_URL}
      AI_MODEL: ${AI_MODEL}
      AI_TIMEOUT: ${AI_TIMEOUT}
      AI_MAX_ATTEMPTS: ${AI_MAX_ATTEMPTS}
      AI_BASE_RETRY_DELAY: ${AI_BASE_RETRY_DELAY}
      INTERNAL_UPDATES_QUEUE_NAME: ${INTERNAL_UPDATES_QUEUE_NAME}
      AI_CLIENT_TYPE: ${AI_CLIENT_TYPE}
      PUSHGATEWAY_URL: http://pushgateway:9091
    secrets:
      - source: db_password
        target: /run/secrets/db_password
      - source: ai_api_key
        target: /run/secrets/ai_api_key
    networks:
      - novel_network
    # Нет healthcheck, так как это воркер

  # Сервис WebSocket уведомлений
  websocket-service:
    build:
      context: .
      dockerfile: websocket-service/Dockerfile
    image: novel-websocket-service:latest
    container_name: novel_websocket_service
    depends_on:
      - rabbitmq
      # Зависимость от auth-service неявная (нужен для валидации токена)
    environment:
      <<: *env-vars # Наследуем общие переменные
      PORT: ${WEBSOCKET_SERVER_PORT} # Внутренний порт сервиса
      METRICS_PORT: ${WEBSOCKET_METRICS_PORT} # Внутренний порт для метрик
      RABBITMQ_URL: amqp://${RABBITMQ_USER}:${RABBITMQ_PASS}@rabbitmq:5672/ # URL для Docker
      SERVICE_ID: websocket-service
      AUTH_SERVICE_URL: http://auth-service:8081 # Добавляем URL для auth-service
    secrets:
      - source: jwt_secret
        target: /run/secrets/jwt_secret
    ports: # Публикуем порт метрик для Prometheus
      - "${WEBSOCKET_METRICS_PORT}:${WEBSOCKET_METRICS_PORT}"
    # labels: # <<< УДАЛЯЕМ LABELS ПОЛНОСТЬЮ
      # --- ВРЕМЕННО УБИРАЕМ ВСЕ МЕТКИ --- 
      # - "traefik.enable=true"
      # - "traefik.http.routers.websocket-ws.rule=Path(`/ws`)"
      # - "traefik.http.routers.websocket-ws.entrypoints=web" # Используем точку входа 'web' (порт 8080)
      # - "traefik.http.services.websocket-service.loadbalancer.server.port=${WEBSOCKET_SERVER_PORT:-8083}"
    networks:
      - novel_network

  # Сервис Уведомлений (Push)
  notification-service:
    build:
      context: .
      dockerfile: notification-service/Dockerfile # Путь к Dockerfile сервиса
    image: novel-notification-service:latest
    container_name: novel_notification_service
    depends_on:
      - rabbitmq
      # Добавьте зависимость от сервиса, предоставляющего токены, если он есть
      # - auth-service
    environment:
      <<: *env-vars # Наследуем общие переменные
      RABBITMQ_URI: amqp://${RABBITMQ_USER}:${RABBITMQ_PASS}@rabbitmq:5672/ # URL для Docker
      SERVICE_ID: notification-service
      # Переменные конфигурации notification-service
      PUSH_QUEUE_NAME: ${PUSH_QUEUE_NAME:-push_notifications}
      WORKER_CONCURRENCY: ${NOTIFICATION_WORKER_CONCURRENCY:-10}
      AUTH_SERVICE_URL: http://auth-service:8081 # <<< Используем AUTH_SERVICE_URL для URL сервиса токенов
      # --- Пути к секретам ВНУТРИ контейнера --- 
      FCM_CREDENTIALS_PATH: /run/secrets/firebase_credentials # <<< Путь к секрету FCM
      APNS_KEY_ID: ${APNS_KEY_ID:-}
      APNS_TEAM_ID: ${APNS_TEAM_ID:-}
      APNS_KEY_PATH: /run/secrets/apns_key # <<< Путь к секрету APNS
      APNS_TOPIC: ${APNS_TOPIC:-}
      # --- Порт для Health Check --- 
      HEALTH_CHECK_PORT: ${NOTIFICATION_HEALTH_PORT:-8088} # <<< Порт для /health
    secrets:
      - source: firebase_credentials # <<< Имя внешнего секрета Docker Swarm/Compose
        target: /run/secrets/firebase_credentials
      # - source: apns_key # <<< Закомментировано, т.к. APNS пока не используется
      #   target: /run/secrets/apns_key
      - source: inter_service_secret # Пример секрета для связи с TokenService
        target: /run/secrets/inter_service_secret
    healthcheck: # <<< Добавляем Healthcheck
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:${NOTIFICATION_HEALTH_PORT:-8088}/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s # Даем время на запуск HTTP сервера
    networks:
      - novel_network

# Новые сервисы для мониторинга
  prometheus:
    image: prom/prometheus:latest
    container_name: novel_prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml # Конфигурация Prometheus
      - prometheus_data:/prometheus # Хранилище данных Prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    networks:
      - novel_network
    depends_on:
      - admin-service
      - auth-service
      - rabbitmq-exporter
      - pushgateway

  grafana:
    image: grafana/grafana:latest
    container_name: novel_grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana # Хранилище данных Grafana
      # Опционально: можно заранее создать provisioning для datasources и dashboards
      # - ./grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_USER=admin # Учетные данные по умолчанию
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_AUTH_DISABLE_LOGIN_FORM=false # Оставляем форму входа
      # GF_INSTALL_PLUGINS: можно указать плагины для установки
    networks:
      - novel_network
    depends_on:
      - prometheus

# --- Добавляем RabbitMQ Exporter --- #
  rabbitmq-exporter:
    image: kbudde/rabbitmq-exporter:latest # Возвращаемся к kbudde, но с тегом latest
    container_name: novel_rabbitmq_exporter
    environment:
      - RABBIT_URL=http://rabbitmq:15672
      - RABBIT_USER=${RABBITMQ_USER}
    ports:
      - "9419:9419" # Публикуем порт для Prometheus
    networks:
      - novel_network
    depends_on:
      - rabbitmq

# Сервис для раздачи статического лендинга
  landing-page:
    image: nginx:alpine # Используем легкий образ Nginx
    container_name: novel_landing_page # Имя контейнера (опционально)
    volumes:
      # Подключаем папку с лендингом в директорию веб-сервера Nginx
      - ./landing-page:/usr/share/nginx/html:ro # Read-only, т.к. Nginx только читает
    networks:
      - novel_network # Подключаем к той же сети, что и Traefik
    deploy: # Политика перезапуска (можно настроить по желанию)
      restart_policy:
        condition: any

# <<< ДОБАВЛЕНО: Сервис Pushgateway >>>
  pushgateway:
    image: prom/pushgateway:latest
    container_name: novel_pushgateway
    ports:
      - "9091:9091" # Публикуем порт для доступа (и для Prometheus)
    networks:
      - novel_network
    restart: unless-stopped # Перезапускать, если не остановлен вручную
# <<< КОНЕЦ ДОБАВЛЕНИЯ >>>

# Сервис Image Generator (Воркер)
  image-generator:
    build:
      context: .
      dockerfile: image-generator/Dockerfile # Убедись, что путь верный
    image: novel-image-generator:latest
    container_name: novel_image_generator
    depends_on:
      - rabbitmq
    volumes:
      # <<< ДОБАВЛЕНО: Подключаем том с изображениями >>>
      # Путь /app/generated_images - куда сервис будет сохранять файлы
      - generated-images-data:/app/generated_images
    environment:
      <<: *env-vars # Наследуем общие переменные, если нужно
      RABBITMQ_URL: amqp://${RABBITMQ_USER}:${RABBITMQ_PASS}@rabbitmq:5672/ # URL для Docker
      SERVICE_ID: image-generator-service
      # <<< ДОБАВЛЕНО: Переменные для image-generator >>>
      IMAGE_SAVE_PATH: /app/generated_images # Путь сохранения внутри контейнера
      IMAGE_PUBLIC_BASE_URL: https://crion.space/generated-images # Базовый URL для формирования ссылки
      IMAGE_PROMPT_STYLE_SUFFIX: ${IMAGE_PROMPT_STYLE_SUFFIX} # <<< Строка для добавления к промптам
      PUSHGATEWAY_URL: http://pushgateway:9091 # <<< URL для Pushgateway внутри Docker
      # <<< ИЗМЕНЕНО: URL для sana-api-wrapper, работающего на хосте >>>
      SANA_API_URL: http://host.docker.internal:8000
      # Добавить другие нужные переменные (логгирование, очереди и т.д.)
      # Например, имя очереди для задач и результатов:
      IMAGE_GENERATOR_TASK_QUEUE: ${IMAGE_GENERATOR_TASK_QUEUE:-image_generation_tasks}
      IMAGE_GENERATOR_RESULT_QUEUE: ${IMAGE_GENERATOR_RESULT_QUEUE:-image_generation_results}
    networks:
      - novel_network
    restart: unless-stopped

# <<< ДОБАВЛЕНО: Сервис для раздачи сгенерированных изображений >>>
  image-server:
    image: nginx:alpine
    container_name: novel_image_server
    volumes:
      # Подключаем том с изображениями в директорию Nginx
      # Путь /usr/share/nginx/html/generated-images будет доступен по HTTP
      - generated-images-data:/usr/share/nginx/html/generated-images:ro # Read-only
    networks:
      - novel_network
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  rabbitmq_data:
  prometheus_data: {} # Volume для данных Prometheus
  grafana_data: {} # Volume для данных Grafana
  # Добавляем том для данных Traefik (если нужно хранить SSL сертификаты и т.д.)
  # traefik_data:
  # <<< ДОБАВЛЕНО: Том для хранения сгенерированных изображений >>>
  generated-images-data: {}

# Определяем сеть, чтобы сервисы могли общаться друг с другом по именам
networks:
  novel_network:
    driver: overlay

# Объявляем все используемые секреты как внешние
secrets:
  db_password:
    external: true
  jwt_secret:
    external: true
  password_pepper:
    external: true
  inter_service_secret:
    external: true
  ai_api_key:
    external: true
  # --- Добавляем внешние секреты для ключей --- 
  firebase_credentials:
    external: true
  # apns_key: # <<< Закомментировано, т.к. APNS пока не используется
  #   external: true
  # Добавляем внешний секрет для ключа APNS, если используется
  #